{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise Sheet 5, Task 3\n",
    "In this assignment, we will implement a neural network “library”, using Python and Numpy. The tool is inspired by PyTorch’s implementation.\n",
    "\n",
    "This week, we will implement Dropout regularisation. For this, we implement a new module ('forward' and 'backward' function) that is initialised with a parameter p, denoting the probability that a weight is dropped. We also need to remember to to scale the resulting weights to make up for the missing weights.\n",
    "\n",
    "We modify the implementation of the NeuralNetwork to include dropout with a specified rate (p:float=0.5 in the constructor) after every hidden layer! We can confirm the random cancellation of weights, since each run of the program will result in different results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "# This is the only fully new part\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        self.mask = np.random.rand(*x.shape) > self.p\n",
    "        # Scale the mask to even out missing neurons\n",
    "        x = x * self.mask / self.p\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad: np.array = np.array([[1]])) -> np.array:\n",
    "        # Scale the mask to even out missing neurons\n",
    "        return grad * self.mask / self.p\n",
    "\n",
    "# Once again, we take all of this from previous weeks, only the Neural Network itself will be altered, not the individual layers.\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def non_rounded_sigmoid(self,x : np.array) -> np.array:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, x: np.array, grad: np.array = np.array([[1]])) -> np.array:\n",
    "        return grad * (self.forward(x) * (1 - self.forward(x)))\n",
    "\n",
    "class MeanSquaredError:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, y_pred: np.array, y_true: np.array) -> float:\n",
    "        return np.mean(0.5 * (y_true - y_pred) ** 2)\n",
    "\n",
    "    def backward(self, y_pred: np.array, y_true: np.array, grad: np.array = np.array([[1]])) -> np.array:\n",
    "        return  grad * (y_pred - y_true)\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.weights = np.random.randn(self.input_size, self.output_size)\n",
    "        self.bias = np.zeros((1, self.output_size))\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        return np.matmul(x, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, x: np.array, grad: np.array = np.array([[1]])) -> Tuple[np.array,np.array,np.array]:\n",
    "        x_grad = np.matmul(grad, self.weights.T)\n",
    "        W_grad = np.matmul(x.T, grad)\n",
    "        b_grad = grad\n",
    "\n",
    "        return x_grad, W_grad, b_grad\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 output_size: int,\n",
    "                 hidden_sizes: List[int],\n",
    "                 activation=Sigmoid,\n",
    "                 dropout:float =0.5 ):\n",
    "        self.activ_inputs = None\n",
    "        self.layer_inputs = None\n",
    "        s = [input_size] + hidden_sizes + [output_size]\n",
    "        self.layers = [FullyConnectedLayer(s[i], s[i+1]) for i in range(len(s) - 1)]\n",
    "        self.dropouts = [Dropout(dropout) for i in range(len(s) - 2)]\n",
    "        self.activation = activation()\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        # we need to edit this function to cache our inputs and outputs for each layer during the forward passe!\n",
    "        self.layer_inputs = []\n",
    "        self.activ_inputs = []\n",
    "\n",
    "        for layer,dropout in zip(self.layers[:-1],self.dropouts):\n",
    "            self.layer_inputs.append(x)\n",
    "            x = layer.forward(x)\n",
    "            self.activ_inputs.append(x)\n",
    "            x = self.activation.forward(x)\n",
    "            # Dropout Layer\n",
    "            x = dropout.forward(x)\n",
    "\n",
    "        #The last layer should not be using an activation function\n",
    "        self.layer_inputs.append(x)\n",
    "        x = self.layers[-1].forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, x: np.array, grad: np.array = np.array([[1]])) -> Tuple[np.array]:\n",
    "        W_grads = []\n",
    "        b_grads = []\n",
    "\n",
    "        # Backward pass for the last layer\n",
    "        grad, W_grad, b_grad = self.layers[-1].backward(self.layer_inputs[-1], grad)\n",
    "        W_grads.append(W_grad)\n",
    "        b_grads.append(b_grad)\n",
    "\n",
    "        # Backward pass for the remaining layers\n",
    "        for i in reversed(range(len(self.activ_inputs))):\n",
    "            # Dropout Layer\n",
    "            grad = self.dropouts[i].backward(grad)\n",
    "            grad = self.activation.backward(self.activ_inputs[i], grad)\n",
    "            grad, W_grad, b_grad = self.layers[i].backward(self.layer_inputs[i], grad)\n",
    "            W_grads.append(W_grad)\n",
    "            b_grads.append(b_grad)\n",
    "\n",
    "        return grad, list(reversed(W_grads)), list(reversed(b_grads))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T11:57:40.306296198Z",
     "start_time": "2023-06-16T11:57:40.261442682Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Implementation\n",
    "\n",
    "Running this cell several times should now yield different results, as the dropout layer will randomly drop neurons. Don't worry if everything becomes 0, as we are using a very small network, and the dropout layer might drop all neurons."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[0.6791787]]\n",
      "Loss: 0.23064185270678947\n",
      "Gradients of the first layer: \n",
      "\n",
      "W1:\n",
      "[[0.14798964 0.        ]\n",
      " [0.14798964 0.        ]], \n",
      "\n",
      "b1: \n",
      "[[0.14798964 0.        ]]\n",
      "\n",
      "Gradients of the second layer: \n",
      "\n",
      "W2:\n",
      "[[0.92256741]\n",
      " [0.        ]], \n",
      "\n",
      "b2 \n",
      "[[0.6791787]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Network Initialization\n",
    "net = NeuralNetwork(2, 1, [2], Sigmoid)\n",
    "\n",
    "# Setting the layer weights\n",
    "net.layers[0].weights = np.array([[0.5, 0.75], [0.25, 0.25]])\n",
    "net.layers[1].weights = np.array([[0.5], [0.5]])\n",
    "\n",
    "# Loss\n",
    "loss_function = MeanSquaredError()\n",
    "\n",
    "# Input\n",
    "x = np.array([[1, 1]])\n",
    "y = np.array([[0]])\n",
    "\n",
    "# Forward Pass\n",
    "pred = net.forward(x)\n",
    "\n",
    "# Loss Calculation\n",
    "loss = loss_function.forward(pred, y)\n",
    "\n",
    "print(f\"Prediction: {pred}\")\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# Backward Pass\n",
    "grad = loss_function.backward(pred, y)\n",
    "grad, W_grads, b_grads = net.backward(x, grad)\n",
    "\n",
    "print(f\"Gradients of the first layer: \\n\\nW1:\\n{W_grads[0]}, \\n\\nb1: \\n{b_grads[0]}\\n\")\n",
    "print(f\"Gradients of the second layer: \\n\\nW2:\\n{W_grads[1]}, \\n\\nb2 \\n{b_grads[1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T11:57:40.307220134Z",
     "start_time": "2023-06-16T11:57:40.306543396Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
