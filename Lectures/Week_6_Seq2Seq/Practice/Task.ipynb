{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise Sheet 6, Task 3\n",
    "In this assignment, we will implement a neural network “library”, using Python and Numpy. The tool is inspired by PyTorch’s implementation.\n",
    "\n",
    "This week, we will adapt the optimisers from the first assignment to train our neural network.\n",
    "\n",
    "In the first assignment, we have already implemented some common optimisers for neural networks (SGD, Nesterov Momentum and Adam).\n",
    "\n",
    "Now we will add optimizer functions for SGD, simple Momentum, and Adam to our framework. We the classes 'SGD', 'Momentum', and 'Adam' to our code. Their constructor are provided with the instantiated NeuralNetwork object and the necessary hyper-parameters such as the learning rate. The classes will offer a method called update that takes the weight and bias gradients from the gradient calculation. This method takes one step of the corresponding optimizer and updated the weights and biases of the given neural network model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# As always, we keep the stuff from previous weeks, so that the notebook can be run on its own. In your code, you can (and likely should)\n",
    "# implement the classes in different files and import them were you need them.\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "#(you need to install this using `pip3 install tqdm`). This is  special version for jupyter notebooks,\n",
    "# in standard python code you can simply use 'import tqdm'\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        self.mask = np.random.rand(*x.shape) > self.p\n",
    "        # Scale the mask to even out missing neurons\n",
    "        x = x * self.mask / self.p\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad: np.array = np.array([[1]])) -> np.array:\n",
    "        # Scale the mask to even out missing neurons\n",
    "        return grad * self.mask / self.p\n",
    "\n",
    "# Once again, we take all of this from previous weeks, only the Neural Network itself will be altered, not the individual layers.\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def non_rounded_sigmoid(self,x : np.array) -> np.array:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, x: np.array, grad: np.array = np.array([[1]])) -> np.array:\n",
    "        return grad * (self.forward(x) * (1 - self.forward(x)))\n",
    "\n",
    "class MeanSquaredError:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, y_pred: np.array, y_true: np.array) -> float:\n",
    "        return np.mean(0.5 * (y_true - y_pred) ** 2)\n",
    "\n",
    "    def backward(self, y_pred: np.array, y_true: np.array, grad: np.array = np.array([[1]])) -> np.array:\n",
    "        return  grad * (y_pred - y_true)\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.weights = np.random.randn(self.input_size, self.output_size)\n",
    "        self.bias = np.zeros((1, self.output_size))\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        return np.matmul(x, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, x: np.array, grad: np.array = np.array([[1]])) -> Tuple[np.array,np.array,np.array]:\n",
    "        x_grad = np.matmul(grad, self.weights.T)\n",
    "        W_grad = np.matmul(x.T, grad)\n",
    "        b_grad = grad\n",
    "\n",
    "        return x_grad, W_grad, b_grad\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 output_size: int,\n",
    "                 hidden_sizes: List[int],\n",
    "                 activation=Sigmoid,\n",
    "                 dropout:float =0.5 ):\n",
    "        self.activ_inputs = None\n",
    "        self.layer_inputs = None\n",
    "        s = [input_size] + hidden_sizes + [output_size]\n",
    "        self.layers = [FullyConnectedLayer(s[i], s[i+1]) for i in range(len(s) - 1)]\n",
    "        self.dropouts = [Dropout(dropout) for i in range(len(s) - 2)]\n",
    "        self.activation = activation()\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        # we need to edit this function to cache our inputs and outputs for each layer during the forward passe!\n",
    "        self.layer_inputs = []\n",
    "        self.activ_inputs = []\n",
    "\n",
    "        for layer,dropout in zip(self.layers[:-1],self.dropouts):\n",
    "            self.layer_inputs.append(x)\n",
    "            x = layer.forward(x)\n",
    "            self.activ_inputs.append(x)\n",
    "            x = self.activation.forward(x)\n",
    "            # Dropout Layer\n",
    "            x = dropout.forward(x)\n",
    "\n",
    "        #The last layer should not be using an activation function\n",
    "        self.layer_inputs.append(x)\n",
    "        x = self.layers[-1].forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, x: np.array, grad: np.array = np.array([[1]])) -> Tuple[np.array]:\n",
    "        W_grads = []\n",
    "        b_grads = []\n",
    "\n",
    "        # Backward pass for the last layer\n",
    "        grad, W_grad, b_grad = self.layers[-1].backward(self.layer_inputs[-1], grad)\n",
    "        W_grads.append(W_grad)\n",
    "        b_grads.append(b_grad)\n",
    "\n",
    "        # Backward pass for the remaining layers\n",
    "        for i in reversed(range(len(self.activ_inputs))):\n",
    "            # Dropout Layer\n",
    "            grad = self.dropouts[i].backward(grad)\n",
    "            grad = self.activation.backward(self.activ_inputs[i], grad)\n",
    "            grad, W_grad, b_grad = self.layers[i].backward(self.layer_inputs[i], grad)\n",
    "            W_grads.append(W_grad)\n",
    "            b_grads.append(b_grad)\n",
    "\n",
    "        return grad, list(reversed(W_grads)), list(reversed(b_grads))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, nn:object, lr:float):\n",
    "        self.nn = nn\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self,W_grads: List[np.array],b_grads: List[np.array]):\n",
    "        for i in range(len(self.nn.layers)):\n",
    "            self.nn.layers[i].weights -= self.lr * W_grads[i]\n",
    "            self.nn.layers[i].bias -= self.lr * b_grads[i]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, nn: object, lr: float, mu: float):\n",
    "        self.v_W = [0] * len(nn.layers)\n",
    "        self.v_b = [0] * len(nn.layers)\n",
    "\n",
    "        self.nn = nn\n",
    "        self.lr = lr\n",
    "        self.mu = mu\n",
    "\n",
    "    def update(self, W_grads: List[np.array], b_grads: List[np.array]):\n",
    "        for i in range(len(self.nn.layers)):\n",
    "            self.v_W[i] = self.mu * self.v_W[i] - self.lr * W_grads[i]\n",
    "            self.v_b[i] = self.mu * self.v_b[i] - self.lr * b_grads[i]\n",
    "\n",
    "            self.nn.layers[i].weights += self.v_W[i]\n",
    "            self.nn.layers[i].bias += self.v_b[i]\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self,nn: object, lr: float, beta1: float, beta2: float):\n",
    "        self.m_W = [0] * len(nn.layers)\n",
    "        self.m_b = [0] * len(nn.layers)\n",
    "\n",
    "        self.v_W = [0] * len(nn.layers)\n",
    "        self.v_b = [0] * len(nn.layers)\n",
    "\n",
    "        self.nn = nn\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "\n",
    "    def update(self, W_grads: List[np.array], b_grads: List[np.array]):\n",
    "        for i in range(len(self.nn.layers)):\n",
    "            self.m_W[i] = self.beta1 * self.m_W[i] + (1 - self.beta1) * W_grads[i]\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * b_grads[i]\n",
    "\n",
    "            self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * W_grads[i] ** 2\n",
    "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * b_grads[i] ** 2\n",
    "\n",
    "            self.nn.layers[i].weights -= self.lr * self.m_W[i] / (np.sqrt(self.v_W[i]) + 1e-8)\n",
    "            self.nn.layers[i].bias -= self.lr * self.m_b[i] / (np.sqrt(self.v_b[i]) + 1e-8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Implementation\n",
    "\n",
    "Running this cell several times should now yield different results, as the dropout layer will randomly drop neurons. Don't worry if everything becomes 0, as we are using a very small network, and the dropout layer might drop all neurons."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Network Initialization\n",
    "net = NeuralNetwork(2, 1, [2], Sigmoid)\n",
    "\n",
    "# Setting the layer weights\n",
    "net.layers[0].weights = np.array([[0.5, 0.75], [0.25, 0.25]])\n",
    "net.layers[1].weights = np.array([[0.5], [0.5]])\n",
    "\n",
    "# Loss\n",
    "loss_function = MeanSquaredError()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = SGD(net, lr=0.1)\n",
    "# optimizer = Momentum(net, lr=0.1, mu=0.0)\n",
    "# optimizer = Adam(net, 0.001, 0.9, 0.999)\n",
    "\n",
    "# XOR Dataset\n",
    "inputs = [\n",
    "    (np.array([[0, 0]]), np.array([0])),\n",
    "    (np.array([[0, 1]]), np.array([1])),\n",
    "    (np.array([[1, 0]]), np.array([1])),\n",
    "    (np.array([[0, 0]]), np.array([0]))\n",
    "]\n",
    "\n",
    "epochs = 50000\n",
    "\n",
    "# tqdm (you need to install it using `pip3 install tqdm`) will show a progress bar for the loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # It is always a good idea to shuffle the dataset\n",
    "    np.random.shuffle(inputs)\n",
    "\n",
    "    for x,y in inputs:\n",
    "        # Forward Pass\n",
    "        pred = net.forward(x)\n",
    "\n",
    "        # Loss Calculation\n",
    "        loss = loss_function.forward(pred, y)\n",
    "\n",
    "        # Backward Pass\n",
    "        grad = loss_function.backward(pred, y)\n",
    "        grad, W_grads, b_grads = net.backward(x, grad)\n",
    "\n",
    "        # Update\n",
    "        optimizer.update(W_grads, b_grads)\n",
    "\n",
    "# Test that the network has learned something\n",
    "for x, y in inputs:\n",
    "    # Forward Pass\n",
    "    pred = net.forward(x)\n",
    "    print(\"Input: {}, Target: {}, Prediction: {:.4f}\".format(x, y, pred[0][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
