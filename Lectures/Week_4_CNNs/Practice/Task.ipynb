{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise Sheet 4, Task 3\n",
    "\n",
    "In this assignment, we will implement a neural network “library”, using Python and Numpy. The tool is inspired by PyTorch’s implementation.\n",
    "\n",
    "In the lecture, we have covered vectorised backpropagation in detail, that we also want to use in this exercise for efficiency."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backward Pass\n",
    "We now implement the ```backward``` functions for each of the module classes implemented last week. Each ```backward``` function gets the input to the function as well as the backpropagating gradient and will output the new gradient for this module. For ```FullyConnectedLayer```, we return a tuple with the gradient w.r.t. the input, the gradient w.r.t. the weights, and the gradient w.r.t. the bias. ```NeuralNetwork``` returns a tuple with the gradient w.r.t. the input, a list of gradients w.r.t. the weights of each layer, and a list of gradients w.r.t. the biases of each layer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#most of this is copied from last week ;)\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def non_rounded_sigmoid(self,x : np.array) -> np.array:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, x: np.array, grad: np.array = np.array([[1]])) -> np.array:\n",
    "        return grad * (self.forward(x) * (1 - self.forward(x)))\n",
    "\n",
    "class MeanSquaredError:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, y_pred: np.array, y_true: np.array) -> float:\n",
    "        return np.mean(0.5 * (y_true - y_pred) ** 2)\n",
    "\n",
    "    def backward(self, y_pred: np.array, y_true: np.array, grad: np.array = np.array([[1]])) -> np.array:\n",
    "        return  grad * (y_pred - y_true)\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.weights = np.random.randn(self.input_size, self.output_size)\n",
    "        self.bias = np.zeros((1, self.output_size))\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        return np.matmul(x, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, x: np.array, grad: np.array = np.array([[1]])) -> Tuple[np.array,np.array,np.array]:\n",
    "        x_grad = np.matmul(grad, self.weights.T)\n",
    "        W_grad = np.matmul(x.T, grad)\n",
    "        b_grad = grad\n",
    "\n",
    "        return x_grad, W_grad, b_grad\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 output_size: int,\n",
    "                 hidden_sizes: List[int],\n",
    "                 activation=Sigmoid):\n",
    "        self.activ_inputs = None\n",
    "        self.layer_inputs = None\n",
    "        s = [input_size] + hidden_sizes + [output_size]\n",
    "        self.layers = [FullyConnectedLayer(s[i], s[i+1]) for i in range(len(s) - 1)]\n",
    "        self.activation = activation()\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        # we need to edit this function to cache our inputs and outputs for each layer during the forward passe!\n",
    "        self.layer_inputs = []\n",
    "        self.activ_inputs = []\n",
    "\n",
    "        for layer in self.layers[:-1]:\n",
    "            self.layer_inputs.append(x)\n",
    "            x = layer.forward(x)\n",
    "            self.activ_inputs.append(x)\n",
    "            x = self.activation.forward(x)\n",
    "\n",
    "        #The last layer should not be using an activation function\n",
    "        self.layer_inputs.append(x)\n",
    "        x = self.layers[-1].forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, x: np.array, grad: np.array = np.array([[1]])) -> Tuple[np.array]:\n",
    "        W_grads = []\n",
    "        b_grads = []\n",
    "\n",
    "        # Backward pass for the last layer\n",
    "        grad, W_grad, b_grad = self.layers[-1].backward(self.layer_inputs[-1], grad)\n",
    "        W_grads.append(W_grad)\n",
    "        b_grads.append(b_grad)\n",
    "\n",
    "        # Backward pass for the remaining layers\n",
    "        for i in reversed(range(len(self.activ_inputs))):\n",
    "            grad = self.activation.backward(self.activ_inputs[i], grad)\n",
    "            grad, W_grad, b_grad = self.layers[i].backward(self.layer_inputs[i], grad)\n",
    "            W_grads.append(W_grad)\n",
    "            b_grads.append(b_grad)\n",
    "\n",
    "        return grad, list(reversed(W_grads)), list(reversed(b_grads))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T16:59:34.992585472Z",
     "start_time": "2023-06-02T16:59:34.950145098Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Implementation\n",
    "Let's apply our backward pass to the network from last week by adding a few lines after the forward pass:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[0.70511864]]\n",
      "Loss: 0.24859614746399733\n",
      "Gradients of the first layer: \n",
      "\n",
      "W1:\n",
      "[[0.07682091 0.06931737]\n",
      " [0.07682091 0.06931737]], \n",
      "\n",
      "b1: \n",
      "[[0.07682091 0.06931737]]\n",
      "\n",
      "Gradients of the second layer: \n",
      "\n",
      "W2:\n",
      "[[0.47890156]\n",
      " [0.51548303]], \n",
      "\n",
      "b2 \n",
      "[[0.70511864]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Network Initialization\n",
    "net = NeuralNetwork(2, 1, [2], Sigmoid)\n",
    "\n",
    "# Setting the layer weights\n",
    "net.layers[0].weights = np.array([[0.5, 0.75], [0.25, 0.25]])\n",
    "net.layers[1].weights = np.array([[0.5], [0.5]])\n",
    "\n",
    "# Loss\n",
    "loss_function = MeanSquaredError()\n",
    "\n",
    "# Input\n",
    "x = np.array([[1, 1]])\n",
    "y = np.array([[0]])\n",
    "\n",
    "# Forward Pass\n",
    "pred = net.forward(x)\n",
    "\n",
    "# Loss Calculation\n",
    "loss = loss_function.forward(pred, y)\n",
    "\n",
    "print(f\"Prediction: {pred}\")\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# Backward Pass\n",
    "grad = loss_function.backward(pred, y)\n",
    "grad, W_grads, b_grads = net.backward(x, grad)\n",
    "\n",
    "print(f\"Gradients of the first layer: \\n\\nW1:\\n{W_grads[0]}, \\n\\nb1: \\n{b_grads[0]}\\n\")\n",
    "print(f\"Gradients of the second layer: \\n\\nW2:\\n{W_grads[1]}, \\n\\nb2 \\n{b_grads[1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T16:59:34.992815138Z",
     "start_time": "2023-06-02T16:59:34.992441840Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We may now check if the gradient computed by our network is the same as the one computed manually. However, we did use an activation function in the final layer on exercise sheet 2, but didn't do this here, so the results will not match without further alterations. Consider that a bonus exercise ;)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
