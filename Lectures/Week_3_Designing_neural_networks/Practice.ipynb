{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the runs deterministic\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Download necessary components\n",
    "# stored in ~/nltk_data\n",
    "nltk.download([\"stopwords\", \"punkt\"])\n",
    "# ~13 mb stored in ~/gensim-data\n",
    "data = api.load(\"20-newsgroups\")\n",
    "# ~1.6 gb, this takes some time!\n",
    "embedding = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class NewsgroupsDataset(Dataset):\n",
    "    \"\"\"20 Newsgroups Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, data: list, labels: dict, embedding: KeyedVectors):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the size of the dataset\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"Returns a data point (text and label) given an index\"\"\"\n",
    "        text = self.data[idx][\"data\"]\n",
    "        text = self.preprocess(text)\n",
    "        text = torch.from_numpy(text).float()  # network inputs need to be float\n",
    "\n",
    "        label = self.data[idx][\"topic\"]\n",
    "        label = self.labels[label]\n",
    "        label = torch.tensor(label).long()  # label is not a continuous value but class indices\n",
    "\n",
    "        return text, label\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\" Processes raw text into neural network input \"\"\"\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        count = 0\n",
    "\n",
    "        features = np.zeros(self.embedding_dim).astype(np.float32)\n",
    "        for token in tokens:\n",
    "            # continue if token is stopword or missing in vocabulary\n",
    "            if token in self.stopwords or token not in self.embedding:\n",
    "                continue\n",
    "\n",
    "            count += 1\n",
    "            features += self.embedding[token]\n",
    "\n",
    "        return features / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class NewsgroupsModel(nn.Module):\n",
    "    \"\"\"Simple Feedforward Neural Network for 20 Newsgroups\"\"\"\n",
    "\n",
    "    def __init__(self, input_size=300):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_1_size = 2048\n",
    "        self.hidden_2_size = 256\n",
    "        self.num_classes = 20\n",
    "\n",
    "        # nn.Linear is a feedforward layer, i.e. that it captures weights and bias values\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(self.hidden_1_size, self.hidden_2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(self.hidden_2_size, self.num_classes)\n",
    "\n",
    "        # weight initialisation\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)  # => logits\n",
    "\n",
    "        # softmax is not used here as the predefined loss function automatically assigns it\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsgroupsModelDropout(nn.Module):\n",
    "    \"\"\"Simple Feedforward Neural Network for 20 Newsgroups\"\"\"\n",
    "\n",
    "    def __init__(self, input_size=300):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = input_size\n",
    "        self.hidden_1_size = 2048\n",
    "        self.hidden_2_size = 256\n",
    "        self.num_classes = 20\n",
    "\n",
    "        # nn.Linear is a feedforward layer, i.e. that it captures weights and bias values\n",
    "        self.fc1 = nn.Linear(self.input, self.hidden_1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(self.hidden_1_size, self.hidden_2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(self.hidden_2_size, self.num_classes)\n",
    "\n",
    "        # Dropout with 10% chance of dropping a neuron\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        # weight initialisation\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)  # => logits\n",
    "\n",
    "        # softmax is not used here as the predefined loss function automatically assigns it\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsgroupsModelLowLevel(nn.Module):\n",
    "    \"\"\"Simple Feedforward Neural Network for 20 Newsgroups\"\"\"\n",
    "\n",
    "    def __init__(self, input_size=300):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_1_size = 2048\n",
    "        self.hidden_2_size = 256\n",
    "        self.num_classes = 20\n",
    "\n",
    "        self.W1 = nn.Parameter(torch.randn(self.input_size, self.hidden_1_size, requires_grad=True))\n",
    "        self.b1 = nn.Parameter(torch.randn(1, self.hidden_1_size, requires_grad=True))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.W2 = nn.Parameter(torch.randn(self.hidden_1_size, self.hidden_2_size, requires_grad=True))\n",
    "        self.b2 = nn.Parameter(torch.randn(1, self.hidden_2_size, requires_grad=True))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.W3 = nn.Parameter(torch.randn(self.hidden_2_size, self.num_classes, requires_grad=True))\n",
    "        self.b3 = nn.Parameter(torch.randn(1, self.num_classes, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # first hidden layer\n",
    "        x = x @ self.W1 + self.b1\n",
    "        x = self.relu1(x)\n",
    "        # second hidden layer\n",
    "        x = x @ self.W2 + self.b2\n",
    "        x = self.relu2(x)\n",
    "        # output layer\n",
    "        x = x @ self.W3 + self.b3  # => logits\n",
    "\n",
    "        # softmax is not used here as the predefined loss function automatically assigns it\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = [], []\n",
    "labels = set()\n",
    "\n",
    "# split data into train and test set\n",
    "for document in data:\n",
    "    labels.add(document[\"topic\"])\n",
    "    if document[\"set\"] == \"train\":\n",
    "        train_data.append(document)\n",
    "    else:\n",
    "        test_data.append(document)\n",
    "\n",
    "# assign indices to labels\n",
    "labels = {label: index for index, label in enumerate(labels)}\n",
    "\n",
    "train_dataset = NewsgroupsDataset(train_data, labels, embedding)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = NewsgroupsDataset(test_data, labels, embedding)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NewsgroupsModel()\n",
    "# model = NewsgroupsModelDropout()\n",
    "# model = NewsgroupsModelLowLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the loss function and optimisation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimiser\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimiser = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics to check the performance\n",
    "\n",
    "We want to check how the model performs on the train and test datasets while and after training.\n",
    "Therefore we build a little helper that calculates the accuracy of the network's predictions.\n",
    "\n",
    "We need to handle the batches that are used while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    \"\"\"A class to keep track of the accuracy while training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the internal state\"\"\"\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "        \n",
    "    def update(self, output, labels):\n",
    "        \"\"\"\n",
    "        Updates the internal state to later compute the overall accuracy\n",
    "        \n",
    "        output: the output of the network for a batch\n",
    "        labels: the target labels\n",
    "        \"\"\"\n",
    "        _, predicted = torch.max(output.data, 1) # predicted now contains the predicted class index/label\n",
    "        \n",
    "        self.total += labels.size(0)\n",
    "        self.correct += (predicted == labels).sum().item() # .item() gets the number, not the tensor\n",
    "\n",
    "    def compute(self):\n",
    "        return self.correct/self.total\n",
    "    \n",
    "\n",
    "accuracy = Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "We loop over the training dataset multiple times (every full iteration is called an *epoch*).\n",
    "For every batch in the dataset, we calculate the loss of the network output, calculate the gradients by using Autograd's automatic gradient calculation, and update the network parameters using the Adam optimiser we initialised before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    print(\"Starting epoch {}\".format(epoch+1))\n",
    "    \n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # to make a beautiful progress bar\n",
    "    loader = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, data in loader:\n",
    "        # get the data points\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients (else, they are accumulated)\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # forward the data through the network\n",
    "        outputs = model(inputs)\n",
    "        # calculate the loss given the output of the network and the target labels\n",
    "        loss = criterion(outputs, labels)\n",
    "        # calculate the gradients of the network w.r.t. its parameters\n",
    "        loss.backward()\n",
    "        # Let the optimiser take an optimization step using the calculated gradients\n",
    "        optimiser.step()\n",
    "        \n",
    "        running_loss += loss\n",
    "        total += outputs.size(0)\n",
    "\n",
    "        loader.set_description(\"loss: {:.5f}\".format(running_loss/total))\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "We can now use the test set to run inference of our model.\n",
    "We can output resulting predictions or use them for testing how well our model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.reset()\n",
    "\n",
    "# Gradients are calculated on the forward pass for every iteration.\n",
    "# As we do not need gradients now, we can disable the calculation.\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(train_loader):\n",
    "        # get the data points\n",
    "        inputs, labels = data\n",
    "\n",
    "        # forward the data through the network\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        accuracy.update(outputs, labels)\n",
    "        \n",
    "print(\"Accuracy: {:.2f}%\".format(100 * accuracy.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.reset()\n",
    "\n",
    "# Gradients are calculated on the forward pass for every iteration.\n",
    "# As we do not need gradients now, we can disable the calculation.\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader): # now the test_loader\n",
    "        # get the data points\n",
    "        inputs, labels = data\n",
    "\n",
    "        # forward the data through the network\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        accuracy.update(outputs, labels)\n",
    "        \n",
    "print(\"Accuracy: {:.2f}%\".format(100 * accuracy.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "nteract": {
   "version": "0.12.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
