{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Import some basic packages we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import nltk\n",
    "import fileinput\n",
    "from gensim.models import Word2Vec\n",
    "from joblib import cpu_count\n",
    "from pathlib import Path\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from collections import deque\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Helper Functions\n",
    "Define some helper functions. These are just for input processing, that is, splitting the input files into sentences and tokenizing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class InputDirectory(object):\n",
    "    \"\"\"\n",
    "    Provide input for the word2vec model from a directory.\n",
    "    All files in the directory are expected to contain one sentence per line (see @split_folder_into_sentences).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dirname):\n",
    "        \"\"\"\n",
    "        :param dirname: The directory to read sentences from\n",
    "        \"\"\"\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "\n",
    "\n",
    "def split_file_into_sentences(filename: str, output_filename: str, language: str = \"english\"):\n",
    "    \"\"\"\n",
    "    Take a file containing text and convert it to a new file with one sentence per line\n",
    "\n",
    "    :param filename: the file to convert\n",
    "    :param output_filename: the name of the output file\n",
    "    :param language: the language the text in the file is in\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        text = f.read()\n",
    "    sentences = sent_tokenize(text, language=language)\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence.replace(\"\\n\", \" \"))\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "                \n",
    "def split_folder_into_sentences(dirname: str, output_dir: str, language: str = \"english\",\n",
    "                                max_workers=min(10, cpu_count() // 4)):\n",
    "    \"\"\"\n",
    "    Take a folder of files containing text and convert them each of them to a new file with one sentence per\n",
    "    line. The new file will have the same name as the original file and will be stored in the specified output\n",
    "    directory.\n",
    "\n",
    "    :param filename: the folder containing files to convert\n",
    "    :param output_filename: the name of the output folder.\n",
    "    :param language: the language the text in the files are in\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    process_pool = ProcessPoolExecutor(max_workers=max_workers)\n",
    "\n",
    "    jobs = deque()\n",
    "\n",
    "    for file in list(Path(dirname).iterdir()):\n",
    "        if file.name.startswith(\".\"):\n",
    "            continue\n",
    "        split_file_into_sentences(str(file), str(output_dir.joinpath(file.name)),\n",
    "                                language=language)\n",
    "\n",
    "        \n",
    "def tokenize_file(filename: str, output_filename: str, tokenizer=nltk.word_tokenize):\n",
    "    with open(filename, \"r\") as in_file:\n",
    "        with open(output_filename, \"w\") as out_file:\n",
    "            for line in in_file.readlines():\n",
    "                tokens = tokenizer(line)\n",
    "                out_file.write(\" \".join(tokens))\n",
    "                out_file.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "def tokenize_folder(dirname: str, output_dir: str, tokenizer=nltk.word_tokenize,\n",
    "                    max_workers=min(10, cpu_count() // 4)):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    process_pool = ProcessPoolExecutor(max_workers=max_workers)\n",
    "\n",
    "    jobs = deque()\n",
    "\n",
    "    for file in list(Path(dirname).iterdir()):\n",
    "        if file.name.startswith(\".\"):\n",
    "            continue\n",
    "        tokenize_file(str(file), str(output_dir.joinpath(file.name)),\n",
    "                                tokenizer=tokenizer)\n",
    "        \n",
    "class Temporary:\n",
    "    \"\"\" Used for FastText: Temporarily create a single file from an input directory \"\"\"\n",
    "    def __init__(self, input_directory):\n",
    "        self.input_dir = input_directory\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%s_tmp.txt\" % self.input_dir\n",
    "\n",
    "    def __enter__(self):\n",
    "        with open(str(self), 'w') as fout:\n",
    "            fin = fileinput.input(glob.glob(\"%s/*.txt\" % self.input_dir))\n",
    "            for line in fin:\n",
    "                fout.write(line)\n",
    "            fin.close()\n",
    "        return str(self)\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        os.remove(str(self))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Word2Vec\n",
    "This is where the actual training happens. Define the input folder in the first line and some hyperparameters of the model in the following lines.\n",
    "\n",
    "Then, all files in the input folder are split into sentences and tokenized. The last two lines train the model using the gensim package and save it to the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "input_folder = \"hp_en\"\n",
    "\n",
    "ITERATIONS = 20\n",
    "NUM_CPUS = min(cpu_count(), 10)\n",
    "VECTOR_SIZE = 300\n",
    "MINIMUM_TOKEN_OCCURRENCES = 100\n",
    "\n",
    "WORD_REGEX = re.compile(\"^[a-zA-Z_]+$\")\n",
    "\n",
    "def filter_words(tokens):\n",
    "    return filter(WORD_REGEX.match, tokens)\n",
    "\n",
    "def TOKENIZER(x: str):\n",
    "    return nltk.word_tokenize(x.lower())\n",
    "\n",
    "input_sentences = \"%s_sentences\" % input_folder\n",
    "input_tokenized = \"%s_tokenized\" % input_folder\n",
    "model_dir = \"%s_model\" % input_folder\n",
    "\n",
    "if not Path(input_sentences).is_dir():\n",
    "    split_folder_into_sentences(input_folder, input_sentences)\n",
    "if not Path(input_tokenized).is_dir():\n",
    "    tokenize_folder(input_sentences, input_tokenized, tokenizer=TOKENIZER, max_workers=NUM_CPUS)\n",
    "tokens = InputDirectory(input_tokenized)\n",
    "\n",
    "model = Word2Vec(tokens, vector_size=VECTOR_SIZE, workers=NUM_CPUS, epochs=ITERATIONS,\n",
    "                 min_count=MINIMUM_TOKEN_OCCURRENCES)\n",
    "\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Word2Vec\n",
    "Now we can do some simple evaluations on the model.\n",
    "For example, we can find the most similar words to any given word in the corpus. The folliwing axamples assume a model that has been trained on the english Harry Potter corpus with lowercasing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('he', 0.6172397136688232), ('cho', 0.4725334048271179), ('neville', 0.47103187441825867), ('krum', 0.42111584544181824), ('hagrid', 0.419950932264328), ('snape', 0.38373202085494995), ('hermione', 0.38220012187957764), ('parvati', 0.38051149249076843), ('she', 0.37862080335617065), ('ron', 0.37831130623817444)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"harry\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the most similar words to \"Harry\" are \"he\" and some other names. This makes sense, as Harry is the protagonist and very frequently referred to as \"he\".\n",
    "Harry Potter nerds might find it interesting to see that Neville is actually most similar character to Harry ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sir', 0.5541843771934509), ('winky', 0.5175085067749023), ('kreacher', 0.4601425528526306), ('elf', 0.43752405047416687), ('bagman', 0.3867757022380829), ('riddle', 0.37830692529678345), ('nick', 0.3709332346916199), ('snape', 0.36309048533439636), ('potter', 0.35632288455963135), ('he', 0.34559860825538635)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"dobby\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most similar words to \"Dobby\" are \"sir\", Winky\", \"Kreacher\" and \"elf\". \"Sir\" is very frequently used by Dobby, while Dobby, Winky and Kreacher are all elves.\n",
    "\n",
    "Simple calculations also work. For example, we can ask the model which word is to \"woman\" as \"Harry\" is to \"man\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she', 0.3949149250984192),\n",
       " ('hermione', 0.38137519359588623),\n",
       " ('cho', 0.37293750047683716),\n",
       " ('he', 0.352822482585907),\n",
       " ('ginny', 0.3491544723510742),\n",
       " ('parvati', 0.33159440755844116),\n",
       " ('bellatrix', 0.3270253539085388),\n",
       " ('neville', 0.3085140287876129),\n",
       " ('her', 0.28927990794181824),\n",
       " ('lily', 0.2862216532230377)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=(\"harry\", \"woman\"), negative=(\"man\", ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that Hermione is basically a female version of Harry, which seems reasonable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Visualizing the embeddings\n",
    "Rather than just looking at individual similarities, it is often interesting to visualize the complete embedding.\n",
    "Tensorflow provides a nice tool for this under http://projector.tensorflow.org.\n",
    "\n",
    "In order to use this tool, we need to export our embeddings in the format the projector wats to see, that is, one file containing each embedding vector in a separate line and one file containing the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#with open(\"%s_vectors.csv\" %input_folder, \"w\") as vectors:\n",
    "#    with open(\"%s_words.csv\" % input_folder, \"w\") as words:\n",
    "#        for word in model.wv.vocab:\n",
    "#            vectors.write(\"\\t\".join(map(str, model.wv[word].tolist())))\n",
    "#            vectors.write(\"\\n\")\n",
    "#            words.write(word+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization go to http://projector.tensorflow.org and load the two files generated in the previous step.\n",
    "\n",
    "t-sne visualization is usually more interesting. Some Clusters you may find include names, words similar to \"said\", numbers, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "input_folder = \"hp_en\"\n",
    "\n",
    "ITERATIONS = 20\n",
    "VECTOR_SIZE = 300\n",
    "MINIMUM_TOKEN_OCCURRENCES = 100\n",
    "MODEL = \"skipgram\"  # alternaticbow\n",
    "\n",
    "input_sentences = \"%s_sentences\" % input_folder\n",
    "input_tokenized = \"%s_tokenized\" % input_folder\n",
    "model_dir = \"%s_fasttext_model\" % input_folder\n",
    "\n",
    "\n",
    "with Temporary(input_tokenized) as input_file:\n",
    "    fast_text_model = fasttext.train_unsupervised(input_file,\n",
    "                                                  model=MODEL,\n",
    "                                                  dim=VECTOR_SIZE,\n",
    "                                                  minCount=MINIMUM_TOKEN_OCCURRENCES,\n",
    "                                                  epoch=ITERATIONS,\n",
    "                                                  verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"%s_fasttext_vectors.csv\" %input_folder, \"w\") as vectors:\n",
    "    with open(\"%s_fasttext_words.csv\" % input_folder, \"w\") as words:\n",
    "        for word in fast_text_model.get_words():\n",
    "            vectors.write(\"\\t\".join(map(str, fast_text_model.get_word_vector(word))))\n",
    "            vectors.write(\"\\n\")\n",
    "            words.write(word+\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GloVe\n",
    "Note: Installing this is a bit more painful than the other libraries and seems to fail for example on current ARM Macs. The necessary library is called `glove_python`, if you want to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# package name glove_python\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "input_folder = \"hp_en\"\n",
    "\n",
    "ITERATIONS = 20\n",
    "VECTOR_SIZE = 300\n",
    "MINIMUM_TOKEN_OCCURRENCES = 100\n",
    "\n",
    "input_sentences = \"%s_sentences\" % input_folder\n",
    "input_tokenized = \"%s_tokenized\" % input_folder\n",
    "model_dir = \"%s_fasttext_model\" % input_folder\n",
    "\n",
    "\n",
    "\n",
    "corpus_model = Corpus()\n",
    "corpus_model.fit(InputDirectory(input_tokenized), window=10)\n",
    "\n",
    "glove_model = Glove(no_components=VECTOR_SIZE)\n",
    "glove_model.fit(corpus_model.matrix, epochs=ITERATIONS, verbose=True)\n",
    "glove_model.add_dictionary(corpus_model.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"%s_glove_vectors.csv\" %input_folder, \"w\") as vectors:\n",
    "    with open(\"%s_glove_words.csv\" % input_folder, \"w\") as words:\n",
    "        for word in glove_model.dictionary:\n",
    "            vectors.write(\"\\t\".join(map(str, glove_model.word_vectors[glove_model.dictionary[word]])))\n",
    "            vectors.write(\"\\n\")\n",
    "            words.write(word+\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
