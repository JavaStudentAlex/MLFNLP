{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Exercise Sheet 7, Task 3\n",
    "In this assignment, we will implement a neural network “library”, using Python and Numpy. The tool is inspired by PyTorch’s implementation.\n",
    "\n",
    "This week, you will use our implementation to train our own word embeddings on a small corpus.\n",
    "\n",
    "The code below is just the neural network we have built so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# As always, we keep the stuff from previous weeks, so that the notebook can be run on its own. In your code, you can (and likely should)\n",
    "# implement the classes in different files and import them were you need them.\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "#(you need to install this using `pip3 install tqdm`). This is  special version for jupyter notebooks,\n",
    "# in standard python code you can simply use 'import tqdm'\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        self.mask = np.random.rand(*x.shape) > self.p\n",
    "        # Scale the mask to even out missing neurons\n",
    "        x = x * self.mask / self.p\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad: np.array = np.array([[1]])) -> np.array:\n",
    "        # Scale the mask to even out missing neurons\n",
    "        return grad * self.mask / self.p\n",
    "\n",
    "# Once again, we take all of this from previous weeks, only the Neural Network itself will be altered, not the individual layers.\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def non_rounded_sigmoid(self,x : np.array) -> np.array:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, x: np.array, grad: np.array = np.array([[1]])) -> np.array:\n",
    "        return grad * (self.forward(x) * (1 - self.forward(x)))\n",
    "\n",
    "class MeanSquaredError:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, y_pred: np.array, y_true: np.array) -> float:\n",
    "        return np.mean(0.5 * (y_true - y_pred) ** 2)\n",
    "\n",
    "    def backward(self, y_pred: np.array, y_true: np.array, grad: np.array = np.array([[1]])) -> np.array:\n",
    "        return  grad * (y_pred - y_true)\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.weights = np.random.randn(self.input_size, self.output_size)\n",
    "        self.bias = np.zeros((1, self.output_size))\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        return np.matmul(x, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, x: np.array, grad: np.array = np.array([[1]])) -> Tuple[np.array,np.array,np.array]:\n",
    "        x_grad = np.matmul(grad, self.weights.T)\n",
    "        W_grad = np.matmul(x.T, grad)\n",
    "        b_grad = grad\n",
    "\n",
    "        return x_grad, W_grad, b_grad\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 output_size: int,\n",
    "                 hidden_sizes: List[int],\n",
    "                 activation=Sigmoid,\n",
    "                 dropout:float =0.5 ):\n",
    "        self.activ_inputs = None\n",
    "        self.layer_inputs = None\n",
    "        s = [input_size] + hidden_sizes + [output_size]\n",
    "        self.layers = [FullyConnectedLayer(s[i], s[i+1]) for i in range(len(s) - 1)]\n",
    "        self.dropouts = [Dropout(dropout) for i in range(len(s) - 2)]\n",
    "        self.activation = activation()\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        # we need to edit this function to cache our inputs and outputs for each layer during the forward passe!\n",
    "        self.layer_inputs = []\n",
    "        self.activ_inputs = []\n",
    "\n",
    "        for layer,dropout in zip(self.layers[:-1],self.dropouts):\n",
    "            self.layer_inputs.append(x)\n",
    "            x = layer.forward(x)\n",
    "            self.activ_inputs.append(x)\n",
    "            x = self.activation.forward(x)\n",
    "            # Dropout Layer\n",
    "            x = dropout.forward(x)\n",
    "\n",
    "        #The last layer should not be using an activation function\n",
    "        self.layer_inputs.append(x)\n",
    "        x = self.layers[-1].forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, x: np.array, grad: np.array = np.array([[1]])) -> Tuple[np.array]:\n",
    "        W_grads = []\n",
    "        b_grads = []\n",
    "\n",
    "        # Backward pass for the last layer\n",
    "        grad, W_grad, b_grad = self.layers[-1].backward(self.layer_inputs[-1], grad)\n",
    "        W_grads.append(W_grad)\n",
    "        b_grads.append(b_grad)\n",
    "\n",
    "        # Backward pass for the remaining layers\n",
    "        for i in reversed(range(len(self.activ_inputs))):\n",
    "            # Dropout Layer\n",
    "            grad = self.dropouts[i].backward(grad)\n",
    "            grad = self.activation.backward(self.activ_inputs[i], grad)\n",
    "            grad, W_grad, b_grad = self.layers[i].backward(self.layer_inputs[i], grad)\n",
    "            W_grads.append(W_grad)\n",
    "            b_grads.append(b_grad)\n",
    "\n",
    "        return grad, list(reversed(W_grads)), list(reversed(b_grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, nn:object, lr:float):\n",
    "        self.nn = nn\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self,W_grads: List[np.array],b_grads: List[np.array]):\n",
    "        for i in range(len(self.nn.layers)):\n",
    "            self.nn.layers[i].weights -= self.lr * W_grads[i]\n",
    "            self.nn.layers[i].bias -= self.lr * b_grads[i]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, nn: object, lr: float, mu: float):\n",
    "        self.v_W = [0] * len(nn.layers)\n",
    "        self.v_b = [0] * len(nn.layers)\n",
    "\n",
    "        self.nn = nn\n",
    "        self.lr = lr\n",
    "        self.mu = mu\n",
    "\n",
    "    def update(self, W_grads: List[np.array], b_grads: List[np.array]):\n",
    "        for i in range(len(self.nn.layers)):\n",
    "            self.v_W[i] = self.mu * self.v_W[i] - self.lr * W_grads[i]\n",
    "            self.v_b[i] = self.mu * self.v_b[i] - self.lr * b_grads[i]\n",
    "\n",
    "            self.nn.layers[i].weights += self.v_W[i]\n",
    "            self.nn.layers[i].bias += self.v_b[i]\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self,nn: object, lr: float, beta1: float, beta2: float):\n",
    "        self.m_W = [0] * len(nn.layers)\n",
    "        self.m_b = [0] * len(nn.layers)\n",
    "\n",
    "        self.v_W = [0] * len(nn.layers)\n",
    "        self.v_b = [0] * len(nn.layers)\n",
    "\n",
    "        self.nn = nn\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "\n",
    "    def update(self, W_grads: List[np.array], b_grads: List[np.array]):\n",
    "        for i in range(len(self.nn.layers)):\n",
    "            self.m_W[i] = self.beta1 * self.m_W[i] + (1 - self.beta1) * W_grads[i]\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * b_grads[i]\n",
    "\n",
    "            self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * W_grads[i] ** 2\n",
    "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * b_grads[i] ** 2\n",
    "\n",
    "            self.nn.layers[i].weights -= self.lr * self.m_W[i] / (np.sqrt(self.v_W[i]) + 1e-8)\n",
    "            self.nn.layers[i].bias -= self.lr * self.m_b[i] / (np.sqrt(self.v_b[i]) + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now, let us build out tuples form the text8 file  from https://data.deepai.org/text8.zip. Download it and unzip it in the same directory as this notebook. This file has already been preprocessed to some degree, so we can just read it in and tokenize it by splitting on spaces.\n",
    "\n",
    "Since we need the size of our vocabulary, let us construct it while building the tuples. We will use a dictionary to map words to integers, and a list to store the tuples.\n",
    "\n",
    "As the dataset is rather small we can store everything in our working memory. For larger datasets we may want to split the data up into several files and only load the parts we need for training into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a468b741886c4326a86eaa032abcef6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17005207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TextDataset:\n",
    "    def __init__(self,file_path:str,occurence_threshold:int=5):\n",
    "        self.file_path = file_path\n",
    "        self.vocabulary = {}\n",
    "        self.tuples = []\n",
    "\n",
    "        self._build_vocab_and_tuples()\n",
    "\n",
    "        # We filter out all words that occur less than occurrence_threshold times.\n",
    "        self.vocabulary = {k:v for k,v in self.vocabulary.items() if v >= occurence_threshold}\n",
    "\n",
    "        # We also need a mapping between the index of a word in the 1-hot vector and the word itself.\n",
    "        # We also add an <UNK> token for words that are not in our vocabulary, which becomes important if we filter out rare words.\n",
    "        self.idx_to_word = {0: \"<UNK>\"}\n",
    "        self.word_to_idx = {\"<UNK>\": 0}\n",
    "\n",
    "        # We construct the mapping based on the alphabetically sorted vocabulary:\n",
    "        vocab_sorted = sorted(self.vocabulary.keys())\n",
    "        for token in vocab_sorted:\n",
    "            self.idx_to_word[len(self.idx_to_word)] = token\n",
    "            self.word_to_idx[token] = len(self.word_to_idx)\n",
    "        #Let's add the <UNK> token to the vocabulary, so that the size of the vocabulary is the same as the size of the 1-hot vector.\n",
    "        self.vocabulary[\"<UNK>\"] = 0\n",
    "\n",
    "\n",
    "    def _build_vocab_and_tuples(self):\n",
    "        with open(self.file_path) as f:\n",
    "            for line in f:\n",
    "                full_line = line.strip().split()\n",
    "                for idx,word in enumerate(tqdm(full_line)):\n",
    "                    if word not in self.vocabulary:\n",
    "                        self.vocabulary[word] = 1\n",
    "                    else :\n",
    "                        self.vocabulary[word] += 1\n",
    "                    # we want to add the previous 2 words, as well as the following two words to the tuple.\n",
    "                    # Let us simply discard the samples where this is not possible, i.e. the first and last 2 words.\n",
    "                    if idx < 2 or idx > len(full_line) - 3:\n",
    "                        continue\n",
    "                    else:\n",
    "                        self.tuples.append(full_line[idx-2:idx+3])\n",
    "\n",
    "\n",
    "    # To stick close to our inspiration in pytorch, let us implement the __len__ and __getitem__ methods as we would do for a pytorch dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.tuples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tuples[idx]\n",
    "\n",
    "    def vocabulary_size(self):\n",
    "        return len(self.vocabulary)\n",
    "\n",
    "#Here we instantiate our dataset. The second parameter is the minimum number of occurrences a word must have to be included in the vocabulary. This is a simple way to reduce the size of our vocabulary, whitout losing too much information. Depending on the size of your dataset, you may want to increase this number. Also, take a moment to consider the impact this might have on the training process and the resulting embeddings. Are there cases where this might be a bad idea and another approach should be considered?\n",
    "dataset = TextDataset(\"text8\",0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We can now investigate our corpus a little bit. Let's check how big our vocabulary is, how many words we have in total, and let's check and confirm that our tuples look right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 253855\n",
      "Total number of words: 17005207\n",
      "Total number of tuples: 17005203\n",
      "As we would expect, the sum of all word counts is exactly four more than the number of tuples, as we discard the first and last two words in each sentence. Of course this changes as soon as we introduce more preprocessing.\n",
      "\n",
      "Now let's check the first 10 tuples:\n",
      "\n",
      "Tuple 0: ['anarchism', 'originated', 'as', 'a', 'term']\n",
      "Tuple 1: ['originated', 'as', 'a', 'term', 'of']\n",
      "Tuple 2: ['as', 'a', 'term', 'of', 'abuse']\n",
      "Tuple 3: ['a', 'term', 'of', 'abuse', 'first']\n",
      "Tuple 4: ['term', 'of', 'abuse', 'first', 'used']\n",
      "Tuple 5: ['of', 'abuse', 'first', 'used', 'against']\n",
      "Tuple 6: ['abuse', 'first', 'used', 'against', 'early']\n",
      "Tuple 7: ['first', 'used', 'against', 'early', 'working']\n",
      "Tuple 8: ['used', 'against', 'early', 'working', 'class']\n",
      "Tuple 9: ['against', 'early', 'working', 'class', 'radicals']\n",
      "\n",
      "They look good. For good measure, let's also quickly check the most common words in our corpus:\n",
      "\n",
      "the: 1061396\n",
      "of: 593677\n",
      "and: 416629\n",
      "one: 411764\n",
      "in: 372201\n",
      "a: 325873\n",
      "to: 316376\n",
      "zero: 264975\n",
      "nine: 250430\n",
      "two: 192644\n",
      "\n",
      "Pretty much what one would expect from a corpus containing actual text samples. For simplicity, we will not strip stopwords from our corpus today, but we may want to consider it in many cases, to remove noise from the corpus.\n",
      "\n",
      "Finally, let's check the mapping between words and indices, by looking at the first few:\n",
      "\n",
      "Word 0: <UNK>\n",
      "Word 1: a\n",
      "Word 2: aa\n",
      "Word 3: aaa\n",
      "Word 4: aaaa\n",
      "Word 5: aaaaaacceglllnorst\n",
      "Word 6: aaaaaaccegllnorrst\n",
      "Word 7: aaaaaah\n",
      "Word 8: aaaaaalmrsstt\n",
      "Word 9: aaaaaannrstyy\n",
      "\n",
      " :( These do not look like real english words. Additional preprocessing should be considered to remove repeated characters, or to map words to their stems, but we will not do that today.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(\"Vocabulary Size: {}\".format(dataset.vocabulary_size()))\n",
    "print(\"Total number of words: {}\".format(sum(dataset.vocabulary.values())))\n",
    "print(\"Total number of tuples: {}\".format(len(dataset)))\n",
    "print(\"As we would expect, the sum of all word counts is exactly four more than the number of tuples, as we discard the first and last two words in each sentence. Of course this changes as soon as we introduce more preprocessing.\\n\\nNow let's check the first 10 tuples:\\n\")\n",
    "for i in range(10):\n",
    "    print(\"Tuple {}: {}\".format(i,dataset[i]))\n",
    "print(\"\\nThey look good. For good measure, let's also quickly check the most common words in our corpus:\\n\")\n",
    "for word,count in sorted(dataset.vocabulary.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(\"{}: {}\".format(word,count))\n",
    "\n",
    "print(\"\\nPretty much what one would expect from a corpus containing actual text samples. For simplicity, we will not strip stopwords from our corpus today, but we may want to consider it in many cases, to remove noise from the corpus.\\n\\nFinally, let's check the mapping between words and indices, by looking at the first few:\\n\")\n",
    "for i in range(10):\n",
    "    print(\"Word {}: {}\".format(i,dataset.idx_to_word[i]))\n",
    "print(\"\\n :( These do not look like real english words. Additional preprocessing should be considered to remove repeated characters, or to map words to their stems, but we will not do that today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Let's also check some random words from the vocabulary, their indices and counts:\n",
      "\n",
      "Word: andfrance, Index: 9218, Count: 1\n",
      "Word: sensitive, Index: 203200, Count: 425\n",
      "Word: arjuna, Index: 13362, Count: 32\n",
      "Word: hird, Index: 101174, Count: 10\n",
      "Word: renouced, Index: 190533, Count: 1\n",
      "Word: electrostatic, Index: 68825, Count: 65\n",
      "Word: mahaparinirvana, Index: 136744, Count: 4\n",
      "Word: acetaminophen, Index: 1580, Count: 12\n",
      "Word: kiyonaga, Index: 122042, Count: 1\n",
      "Word: halococcus, Index: 95830, Count: 1\n",
      "\n",
      "\n",
      "Now that we have our dataset, we can start and train our model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nLet's also check some random words from the vocabulary, their indices and counts:\\n\")\n",
    "for i in range(10):\n",
    "    word = random.choice(list(dataset.vocabulary.keys()))\n",
    "    print(\"Word: {}, Index: {}, Count: {}\".format(word,dataset.word_to_idx[word],dataset.vocabulary[word]))\n",
    "print(\"\\n\\nNow that we have our dataset, we can start and train our model.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04eb7c7b249b4aa09befa540fd0ac007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6d056d97c0475eafea9146f3f28073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78639/4208183115.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "vocab_size = dataset.vocabulary_size()\n",
    "sample_count = len(dataset)\n",
    "index_list = list(range(sample_count))\n",
    "\n",
    "# Network Initialization\n",
    "net = NeuralNetwork(vocab_size, vocab_size,[32], Sigmoid)\n",
    "\n",
    "# Loss\n",
    "loss_function = MeanSquaredError()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = SGD(net, lr=0.1)\n",
    "# optimizer = Momentum(net, lr=0.1, mu=0.0)\n",
    "# optimizer = Adam(net, 0.001, 0.9, 0.999)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "# tqdm (you need to install it using `pip3 install tqdm`) will show a progress bar for the loop\n",
    "for epoch_idx, epoch in enumerate(tqdm(range(epochs))):\n",
    "    # It is always a good idea to shuffle the dataset.\n",
    "    # Here we shuffle our indices and then iterate over them\n",
    "    np.random.shuffle(index_list)\n",
    "\n",
    "    #training over all samples may take waaaay to long, especially on a CPU, so we will only train on 10k random samples per batch\n",
    "    for i in tqdm(index_list[:10000]):\n",
    "        # get our sample at the index location\n",
    "        sample = dataset[i]\n",
    "        # get our center word and context from that sample and let us construct our 1-hot vector\n",
    "        # first, we build a vector of the correct shape, initialized with zeros\n",
    "        x = np.zeros((1,vocab_size))\n",
    "        # then we set the index of the center word to 1\n",
    "        x[0,dataset.word_to_idx.get(sample[2],0)] = 1\n",
    "        # now we do the same for the context words, we use the get function of dictionaries to return 0 if the word is not in the dictionary, as that is the index of the <UNK> token.\n",
    "        y = np.zeros((1,vocab_size))\n",
    "        y[0,dataset.word_to_idx.get(sample[0],0)] = 1\n",
    "        y[0,dataset.word_to_idx.get(sample[1],0)] = 1\n",
    "        y[0,dataset.word_to_idx.get(sample[3],0)] = 1\n",
    "        y[0,dataset.word_to_idx.get(sample[4],0)] = 1\n",
    "\n",
    "        # Forward Pass\n",
    "        pred = net.forward(x)\n",
    "\n",
    "        # Loss Calculation\n",
    "        loss = loss_function.forward(pred, y)\n",
    "\n",
    "        # Backward Pass\n",
    "        grad = loss_function.backward(pred, y)\n",
    "        grad, W_grads, b_grads = net.backward(x, grad)\n",
    "\n",
    "        # Update\n",
    "        optimizer.update(W_grads, b_grads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This takes forever. Preprocessing can greatly reduce the size of the dataset, since it dictates the size of our input and output layers. One easy way to achieve this is by dropping all rare words from the dataset. I have prepared our dataset to do. Try building the dataset with only words that occur more than 5/50/500 times. Take a look at how the vocabulary changes, and the impact it has on our training. Also, feel free to implement you own preprocessing, dropping rare words is a very simple approach and, while effective, not the only nor always the ideal choice.\n",
    "\n",
    "Once we are dne training our model we can take a look at the embeddings. We can do this by simply looking at the weights of the hidden layer. We can also use the embeddings to find similar words. We can do this by calculating the cosine similarity between the embeddings of two words. Let's try it out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
